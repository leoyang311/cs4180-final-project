{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 1.0349, Validation Accuracy: 0.3750\n",
      "Model saved to iris_model.pt\n",
      "Epoch [2/50], Train Loss: 1.0088, Validation Accuracy: 0.4167\n",
      "Model saved to iris_model.pt\n",
      "Epoch [3/50], Train Loss: 0.9825, Validation Accuracy: 0.5000\n",
      "Model saved to iris_model.pt\n",
      "Epoch [4/50], Train Loss: 0.9557, Validation Accuracy: 0.6250\n",
      "Model saved to iris_model.pt\n",
      "Epoch [5/50], Train Loss: 0.9297, Validation Accuracy: 0.7083\n",
      "Model saved to iris_model.pt\n",
      "Epoch [6/50], Train Loss: 0.9037, Validation Accuracy: 0.6667\n",
      "Epoch [7/50], Train Loss: 0.8774, Validation Accuracy: 0.7083\n",
      "Epoch [8/50], Train Loss: 0.8514, Validation Accuracy: 0.7917\n",
      "Model saved to iris_model.pt\n",
      "Epoch [9/50], Train Loss: 0.8250, Validation Accuracy: 0.8333\n",
      "Model saved to iris_model.pt\n",
      "Epoch [10/50], Train Loss: 0.7989, Validation Accuracy: 0.8333\n",
      "Epoch [11/50], Train Loss: 0.7738, Validation Accuracy: 0.8333\n",
      "Epoch [12/50], Train Loss: 0.7471, Validation Accuracy: 0.8333\n",
      "Epoch [13/50], Train Loss: 0.7228, Validation Accuracy: 0.8333\n",
      "Epoch [14/50], Train Loss: 0.6979, Validation Accuracy: 0.8333\n",
      "Epoch [15/50], Train Loss: 0.6743, Validation Accuracy: 0.8333\n",
      "Epoch [16/50], Train Loss: 0.6515, Validation Accuracy: 0.8333\n",
      "Epoch [17/50], Train Loss: 0.6295, Validation Accuracy: 0.8333\n",
      "Epoch [18/50], Train Loss: 0.6087, Validation Accuracy: 0.8333\n",
      "Epoch [19/50], Train Loss: 0.5891, Validation Accuracy: 0.8333\n",
      "Epoch [20/50], Train Loss: 0.5703, Validation Accuracy: 0.8750\n",
      "Model saved to iris_model.pt\n",
      "Epoch [21/50], Train Loss: 0.5522, Validation Accuracy: 0.8750\n",
      "Epoch [22/50], Train Loss: 0.5360, Validation Accuracy: 0.8750\n",
      "Epoch [23/50], Train Loss: 0.5203, Validation Accuracy: 0.8750\n",
      "Epoch [24/50], Train Loss: 0.5051, Validation Accuracy: 0.8750\n",
      "Epoch [25/50], Train Loss: 0.4914, Validation Accuracy: 0.8750\n",
      "Epoch [26/50], Train Loss: 0.4778, Validation Accuracy: 0.8750\n",
      "Epoch [27/50], Train Loss: 0.4660, Validation Accuracy: 0.8750\n",
      "Epoch [28/50], Train Loss: 0.4540, Validation Accuracy: 0.8750\n",
      "Epoch [29/50], Train Loss: 0.4432, Validation Accuracy: 0.8750\n",
      "Epoch [30/50], Train Loss: 0.4326, Validation Accuracy: 0.8750\n",
      "Epoch [31/50], Train Loss: 0.4224, Validation Accuracy: 0.8750\n",
      "Epoch [32/50], Train Loss: 0.4132, Validation Accuracy: 0.8750\n",
      "Epoch [33/50], Train Loss: 0.4042, Validation Accuracy: 0.8750\n",
      "Epoch [34/50], Train Loss: 0.3960, Validation Accuracy: 0.8750\n",
      "Epoch [35/50], Train Loss: 0.3882, Validation Accuracy: 0.8750\n",
      "Epoch [36/50], Train Loss: 0.3803, Validation Accuracy: 0.8750\n",
      "Epoch [37/50], Train Loss: 0.3727, Validation Accuracy: 0.8750\n",
      "Epoch [38/50], Train Loss: 0.3657, Validation Accuracy: 0.8750\n",
      "Epoch [39/50], Train Loss: 0.3590, Validation Accuracy: 0.8750\n",
      "Epoch [40/50], Train Loss: 0.3528, Validation Accuracy: 0.8750\n",
      "Epoch [41/50], Train Loss: 0.3464, Validation Accuracy: 0.8750\n",
      "Epoch [42/50], Train Loss: 0.3404, Validation Accuracy: 0.8750\n",
      "Epoch [43/50], Train Loss: 0.3348, Validation Accuracy: 0.8750\n",
      "Epoch [44/50], Train Loss: 0.3293, Validation Accuracy: 0.8750\n",
      "Epoch [45/50], Train Loss: 0.3238, Validation Accuracy: 0.8750\n",
      "Epoch [46/50], Train Loss: 0.3187, Validation Accuracy: 0.8750\n",
      "Epoch [47/50], Train Loss: 0.3137, Validation Accuracy: 0.8750\n",
      "Epoch [48/50], Train Loss: 0.3089, Validation Accuracy: 0.8750\n",
      "Epoch [49/50], Train Loss: 0.3041, Validation Accuracy: 0.8750\n",
      "Epoch [50/50], Train Loss: 0.2995, Validation Accuracy: 0.8750\n",
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for IrisNet:\n\tMissing key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"network.0.weight\", \"network.0.bias\", \"network.3.weight\", \"network.3.bias\", \"network.6.weight\", \"network.6.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 259\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# Optionally, you can access the metrics dictionary\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# print(metrics)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 259\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 247\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# Evaluate the saved model using the evaluate_pt_model function\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_pt_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/yangshuntian/cs4180/src/cs4180/final_project/new_dqn_multi/itr1/dqn_trading_agent_AAPL.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 100\u001b[0m, in \u001b[0;36mevaluate_pt_model\u001b[0;34m(model_path, X_test, y_test, scaler, device, visualize)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_path):\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe specified model path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set model to evaluation mode\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel loaded from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for IrisNet:\n\tMissing key(s) in state_dict: \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"network.0.weight\", \"network.0.bias\", \"network.3.weight\", \"network.3.bias\", \"network.6.weight\", \"network.6.bias\". "
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a custom Dataset for the Iris data\n",
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with features and labels.\n",
    "\n",
    "        Args:\n",
    "            features (numpy.ndarray): Feature matrix.\n",
    "            labels (numpy.ndarray): Label vector.\n",
    "        \"\"\"\n",
    "        self.X = torch.tensor(features, dtype=torch.float32)\n",
    "        self.y = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the feature and label at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (feature, label) pair.\n",
    "        \"\"\"\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Define the neural network model\n",
    "class IrisNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        \"\"\"\n",
    "        Initializes the neural network layers.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of input features.\n",
    "            hidden_size (int): Number of neurons in the hidden layer.\n",
    "            num_classes (int): Number of output classes.\n",
    "        \"\"\"\n",
    "        super(IrisNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # First fully connected layer\n",
    "        self.relu = nn.ReLU()                          # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output logits.\n",
    "        \"\"\"\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "def evaluate_pt_model(model_path, X_test, y_test, scaler=None, device=None, visualize=True):\n",
    "    \"\"\"\n",
    "    Evaluates a PyTorch .pt model on the provided test data.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the saved .pt model file.\n",
    "        X_test (numpy.ndarray): Test feature matrix.\n",
    "        y_test (numpy.ndarray): True labels for the test data.\n",
    "        scaler (sklearn.preprocessing.StandardScaler, optional): Scaler used to preprocess the data.\n",
    "            If provided, it will be used to transform the test data.\n",
    "        device (torch.device, optional): Device to perform computation on.\n",
    "            If not provided, it will be set to CUDA if available, else CPU.\n",
    "        visualize (bool, optional): Whether to visualize the confusion matrix.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Using device: {device}')\n",
    "\n",
    "    # Load the Iris dataset for defining the model architecture\n",
    "    # This assumes that the model was trained on the Iris dataset\n",
    "    iris = load_iris()\n",
    "    input_size = X_test.shape[1]    # Number of features\n",
    "    hidden_size = 16                # Must match the hidden size used during training\n",
    "    num_classes = len(np.unique(y_test))  # Number of classes\n",
    "\n",
    "    # Initialize the model and load state_dict\n",
    "    model = IrisNet(input_size, hidden_size, num_classes).to(device)\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"The specified model path does not exist: {model_path}\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    print(f'Model loaded from {model_path}')\n",
    "\n",
    "    # Preprocess the test data\n",
    "    if scaler is not None:\n",
    "        X_test = scaler.transform(X_test)\n",
    "    test_dataset = IrisDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Make predictions\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_predictions.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print('\\nEvaluation Metrics:')\n",
    "    print(f'Accuracy : {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall   : {recall:.4f}')\n",
    "    print(f'F1 Score : {f1:.4f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Visualize the confusion matrix\n",
    "    if visualize:\n",
    "        plt.figure(figsize=(6,5))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=iris.target_names,\n",
    "                    yticklabels=iris.target_names)\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "    # Return metrics as a dictionary\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the evaluation of a saved .pt model.\n",
    "    \"\"\"\n",
    "    # Load the Iris dataset\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Split the data into training and test sets (80% train, 20% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Define model parameters\n",
    "    input_size = X.shape[1]    # Number of features\n",
    "    hidden_size = 16           # Number of neurons in hidden layer\n",
    "    num_classes = len(np.unique(y))  # Number of classes\n",
    "\n",
    "    # Initialize the model, loss function, and optimizer\n",
    "    model = IrisNet(input_size, hidden_size, num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Create Dataset and DataLoader for training\n",
    "    train_dataset = IrisDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Training parameters\n",
    "    num_epochs = 50\n",
    "    best_val_accuracy = 0.0\n",
    "    save_path = 'iris_model.pt'\n",
    "\n",
    "    # For simplicity, we'll use a portion of training data as validation\n",
    "    # Split training data into actual training and validation sets\n",
    "    X_train_actual, X_val, y_train_actual, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    "    )\n",
    "    train_actual_dataset = IrisDataset(X_train_actual, y_train_actual)\n",
    "    train_actual_loader = DataLoader(train_actual_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataset = IrisDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_actual_loader:\n",
    "            optimizer.zero_grad()           # Zero the parameter gradients\n",
    "            outputs = model(inputs)         # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Compute loss\n",
    "            loss.backward()                 # Backward pass\n",
    "            optimizer.step()                # Update weights\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_actual_loader.dataset)\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        all_predictions = []\n",
    "        all_labels_val = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_predictions.extend(preds.numpy())\n",
    "                all_labels_val.extend(labels.numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(all_labels_val, all_predictions)\n",
    "\n",
    "        print(\n",
    "            f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "            f'Train Loss: {epoch_loss:.4f}, '\n",
    "            f'Validation Accuracy: {val_accuracy:.4f}'\n",
    "        )\n",
    "\n",
    "        # Save the model if validation accuracy improves\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f'Model saved to {save_path}')\n",
    "\n",
    "    # Evaluate the saved model using the evaluate_pt_model function\n",
    "    metrics = evaluate_pt_model(\n",
    "        model_path=\"/Users/yangshuntian/cs4180/src/cs4180/final_project/new_dqn_multi/itr1/dqn_trading_agent_AAPL.pt\",\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        scaler=scaler,\n",
    "        visualize=True\n",
    "    )\n",
    "\n",
    "    # Optionally, you can access the metrics dictionary\n",
    "    # print(metrics)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"src/cs4180/final_project/new_dqn_multi/itr1/dqn_trading_agent_AAPL.pt\"\n",
    "\n",
    "def example_basic_evaluation():\n",
    "    # 加载 Iris 数据集\n",
    "    iris = load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "\n",
    "    # 标准化特征\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # 分割数据集为训练集和测试集（80% 训练，20% 测试）\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # 模型文件路径\n",
    "    # model_path = path\n",
    "\n",
    "    # 评估模型\n",
    "    metrics = evaluate_pt_model(\n",
    "        model_path= path,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        scaler=scaler,\n",
    "        visualize=True\n",
    "    )\n",
    "\n",
    "    # 打印评估指标\n",
    "    print(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
